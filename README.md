# AEBT
Self-supervised learning (SSL) typically promotes invariance to data augmentations, effectively learning semantic representations but often inadvertently discarding potentially useful augmentation-specific information. Motivated by principles of memetic computing, this paper proposes a novel hybrid SSL framework named AutoEncoder Barlow Triplets (AEBT), explicitly designed to disentangle semantic content from transformation-specific information. Our approach integrates a dual-encoder structure—capturing invariant semantics and explicit transformations separately—with a lightweight decoder and a redundancy-reduction projection head, collectively optimized via a unified loss function. Specifically, we combine a Barlow Triplets loss to enforce invariance and reduce redundancy, a reconstruction loss to preserve both semantic and transformation signals, and a decorrelation loss to explicitly enforce feature disentanglement. Comprehensive experiments on CIFAR-10 and CIFAR-100 datasets demonstrate that AEBT achieves competitive performance in semantic discrimination and effectively captures augmentation-sensitive features compared to state-of-the-art SSL approaches. Ablation studies further reveal the contributions of each component, highlighting their complementary roles. These findings suggest the potential of hybrid approaches in representation learning, providing insights for future exploration of memetic methodologies in deep learning contexts.
